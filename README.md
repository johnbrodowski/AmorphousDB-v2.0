### **AmorphousDB Protocol v2.0 — A Federated, Self-Deduplicating Knowledge Store**

#### **Abstract**

AmorphousDB is a distributed database protocol where multiple instances — on the same server or across different locations — synchronize and behave as a single logical database. Instances can be added or removed dynamically without downtime.

The core data structure is the **Cogniton**: a unit that combines semantic embeddings (for meaning) with relational edges (for structure) in one entity. This eliminates the awkward split between vector databases and graph databases.

Key features:
- **Federation**: Multiple database instances sync automatically. A company's databases across offices appear as one.
- **Semantic deduplication**: New data is checked against existing data using vector similarity. Near-duplicates are detected and handled, not just exact matches.
- **Broadcast queries**: Queries are sent to all instances; only those with relevant data respond.
- **Cold storage with reconnection**: Unused data moves to archival storage, with graph connections preserved.

---

#### **1. Introduction: The Problem**

Most systems that need both semantic search (vectors) and structured relationships (graphs) end up bolting together separate databases — a vector store like Pinecone alongside a graph database like Neo4j. This creates:

- **Data duplication**: The same information lives in multiple places.
- **Sync complexity**: Keeping the vector and graph representations consistent is error-prone.
- **Query limitations**: You can't easily combine "find semantically similar" with "traverse relationships" in one operation.

AmorphousDB addresses this by storing semantic and relational information together in a single structure.

The v2.0 protocol adds distribution: multiple AmorphousDB instances form a federation, sharing data like BitTorrent peers share files. You can run instances on one server for redundancy, or across data centers for geographic distribution. The federation handles synchronization, deduplication, and query routing.

---

#### **2. Core Architecture: The Cogniton**

The **Cogniton** is the fundamental data unit. It combines what would normally be stored separately in vector and graph databases:

```
Cogniton {
    id:        unique identifier (content hash)
    vector:    semantic embedding (e.g., 768 or 1536 dimensions)
    edges:     list of labeled connections to other Cognitons
    metadata:  source, timestamp, instance origin
}
```

**Fields explained:**

- **`id`**: A hash of the Cogniton's content. This serves as both identifier and integrity check — if the content changes, the hash changes.
- **`vector`**: A numerical representation of the Cogniton's meaning, generated by an embedding model (e.g., sentence-transformers, OpenAI embeddings). Used for similarity search.
- **`edges`**: Labeled, directed links to other Cognitons. Example: `("is_part_of", cogniton_456)`. This is the relational/graph component.
- **`metadata`**: Origin information — which instance created it, when, from what source. Used for sync conflict resolution.

**Spatial mental model**

Think of Cognitons as nodes floating in high-dimensional space. Their position is determined by their vector — semantically similar data clusters together. Edges connect nearby nodes, forming a mesh where each node links to its closest neighbors.

When a node is removed, the graph self-heals: surrounding nodes reconnect to each other, filling the gap. The mesh remains connected because proximity relationships are preserved — if A linked to B and B linked to C, and B is removed, A and C are likely still close enough to link directly.

**The distributed graph** is the collection of all Cognitons across all federated instances. Each instance holds a subset (or full copy, depending on configuration), and they synchronize changes.

**Sharding**: For large deployments, the graph can be partitioned by semantic clusters. Cognitons with similar vectors live on the same shard, making similarity queries local to a shard when possible.

---

#### **3. Ingestion: Adding Data with Deduplication**

When new data is added, AmorphousDB checks for duplicates before inserting:

**Step 1: Generate embedding**
The new data is converted to a vector using the configured embedding model.

**Step 2: Check for duplicates**
The vector is compared against existing Cognitons using similarity search. Two levels of matching:

- **Exact match** (hash collision): The data already exists. Skip insertion.
- **Near match** (vector similarity > threshold, e.g., 0.95): A semantically similar Cogniton exists. Options:
  - Reject the new data
  - Merge with existing (combine edges, keep newer metadata)
  - Insert anyway with a "similar_to" edge linking them
  - Flag for manual review

The appropriate behavior is configurable per deployment.

**Step 3: Find neighbors**
Similarity search identifies the N most similar existing Cognitons. These become candidates for edge creation.

**Step 4: Create edges**
Edges can be:
- Explicitly provided by the caller
- Inferred from similarity (automatic "related_to" edges to nearest neighbors)
- Generated by an optional LLM that suggests relationship types based on content

**Step 5: Insert and sync**
The new Cogniton is inserted locally and broadcast to other instances in the federation. Each instance applies the same deduplication check before accepting.

**Conflict resolution**: If two instances simultaneously add similar data (race condition), the conflict is resolved by timestamp — earliest insertion wins, later duplicates are merged or rejected.

---

#### **4. Query Engine: Broadcast and Collect**

Queries in a federated AmorphousDB use a scatter-gather pattern:

**Step 1: Broadcast**
The query (text or vector) is sent to all instances in the federation. Each instance searches its local data.

**Step 2: Local search**
Each instance performs:
- **Vector similarity search**: Find Cognitons whose vectors are close to the query vector
- **Graph traversal** (optional): From the initial matches, follow edges to find related Cognitons

**Step 3: Collect and merge**
Results from all instances are collected, deduplicated (by Cogniton ID), and ranked by relevance score.

**Fault tolerance**: If an instance is unavailable, the query proceeds with the remaining instances. Results may be incomplete, but the system doesn't fail. The caller can be notified which instances didn't respond.

**Optimization**: For sharded deployments, the query is routed only to shards likely to contain relevant data (based on the query vector's position in the semantic space). This avoids broadcasting to every shard.

---

#### **5. Cold Storage and Pruning**

To manage storage costs, infrequently accessed Cognitons can be moved to cheaper archival storage:

**Usage tracking**
Each Cogniton has an access counter that decays over time. Frequently accessed data stays "hot"; unused data becomes "cold."

**Pruning process**
When a Cogniton's access score falls below a threshold (configurable, e.g., no access in 90 days):

1. The full Cogniton is moved to cold storage (S3, local archive, etc.)
2. A stub remains in the active graph containing:
   - The Cogniton ID
   - A pointer to the cold storage location
   - The vector (so similarity search still works)
3. Edges are preserved — they now point to the stub

**Retrieval**
When a query matches a stub, the full Cogniton is fetched from cold storage and restored to the active graph. Its access counter resets.

**Self-healing on removal**
When a Cogniton is removed (or fully pruned without a stub), the graph reconnects automatically:

1. Find the removed node's neighbors (nodes it was connected to)
2. For each pair of neighbors that are now disconnected, check their vector distance
3. If they're close enough in semantic space, create a direct edge between them

This fills the "hole" left by the removed node. Because edges follow semantic proximity, the mesh stays coherent — nearby nodes remain connected through alternative paths.

**Coordination**
In a federation, pruning decisions are made per-instance. Each instance manages its own hot/cold boundary. A Cogniton might be cold on one instance but hot on another (due to different access patterns).

---

#### **6. Data Integrity and Security**

**Content-addressed storage**
Cogniton IDs are hashes of their content. This provides:
- **Integrity verification**: If data is corrupted or tampered with, the hash won't match
- **Deduplication**: Identical content always produces the same ID
- **Cache-friendly**: Cognitons are immutable once created (edits create new Cognitons)

**Access control**
Each instance can enforce its own access policies:
- Read/write permissions per user or API key
- Namespace isolation (different teams see different subsets)
- Edge-level permissions (some relationships may be restricted)

**Audit logging**
Changes are logged with timestamps and source instance. This provides a history of who added what and when.

**Federation trust**
Instances in a federation must be configured to trust each other. An instance only syncs with explicitly approved peers. This is not a public network — it's a private federation under organizational control.

---

#### **7. Implementation Notes**

**Suggested technology stack** (reference implementation):

| Component | Suggested approach |
|-----------|-------------------|
| Core | C# / .NET |
| Hot storage | SQLite or PostgreSQL per instance |
| Cold storage | S3, local filesystem, or similar |
| Vector index | HNSW (Hierarchical Navigable Small World) for approximate nearest neighbor |
| Embeddings | ONNX runtime with sentence-transformers or similar |
| Federation sync | gRPC or message queue (RabbitMQ, Redis streams) |
| API | REST and/or gRPC endpoints |

**Modules**:
- **Cogniton**: Core data structure with vector, edges, metadata
- **Ingestion**: Embedding generation, deduplication, edge creation
- **Query**: Similarity search, graph traversal, result aggregation
- **Sync**: Instance discovery, change propagation, conflict resolution
- **Pruning**: Background process for cold storage migration

**Not yet benchmarked**: Performance claims require real implementation and testing. Expected benefits are reduced storage through deduplication and pruning, and simplified architecture compared to separate vector + graph databases.

---

#### **8. Summary**

AmorphousDB is a federated database that:

1. **Unifies vectors and graphs** — The Cogniton structure stores semantic embeddings and relational edges together, eliminating the need for separate vector and graph databases.

2. **Federates across instances** — Multiple database instances sync automatically. Add or remove instances as needed. A company's data across locations appears as one logical database.

3. **Deduplicates semantically** — New data is checked against existing data using vector similarity, catching near-duplicates that hash-based deduplication would miss.

4. **Manages storage lifecycle** — Unused data moves to cold storage automatically, with graph connections preserved.

This is a protocol specification. Implementation work is needed to validate the design and measure real-world performance.
